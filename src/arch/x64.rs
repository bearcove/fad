use dynasmrt::{dynasm, DynasmApi, DynasmLabelApi, DynamicLabel, AssemblyOffset};

use crate::context::{CTX_ERROR_CODE, CTX_INPUT_PTR, CTX_INPUT_END};
use crate::recipe::{Op, Recipe, Slot, Width, ErrorTarget};

pub type Assembler = dynasmrt::x64::Assembler;

/// Base frame size.
///
/// - System V AMD64: `rbp + pad + r12 + r13 + r14 + r15` = 48 bytes
/// - Windows x64: `shadow(32) + rbp + pad + r12 + r13 + r14 + r15` = 80 bytes
///
/// The Windows layout reserves 32 bytes at [rsp+0..31] as the callee home
/// (shadow) space required by the Windows x64 ABI before every call.
#[cfg(not(windows))]
pub const BASE_FRAME: u32 = 48;
#[cfg(windows)]
pub const BASE_FRAME: u32 = 80;

/// Emission context — wraps the assembler plus bookkeeping labels.
pub struct EmitCtx {
    pub ops: Assembler,
    pub error_exit: DynamicLabel,
    pub entry: AssemblyOffset,
    /// Total frame size (base + extra, 16-byte aligned).
    pub frame_size: u32,
}

// Register assignments (callee-saved across all platforms):
//   r12 = cached input_ptr
//   r13 = cached input_end
//   r14 = out pointer
//   r15 = ctx pointer
//
// Scratch (caller-saved):
//   rax = fn ptr loads, return values
//   r10, r11 = temporaries
//
// Argument registers for calls to intrinsics:
//   System V AMD64:  arg0=rdi, arg1=rsi, arg2=rdx, arg3=rcx, arg4=r8, arg5=r9
//   Windows x64:     arg0=rcx, arg1=rdx, arg2=r8,  arg3=r9   (4 register args only)

impl EmitCtx {
    /// Create a new EmitCtx. Does not emit any code.
    ///
    /// `extra_stack` is the number of additional bytes the format needs on the
    /// stack (e.g. 32 for JSON's bitset + key_ptr + key_len + peek_byte).
    /// The total frame is rounded up to 16-byte alignment.
    ///
    /// Call `begin_func()` to emit a function prologue.
    pub fn new(extra_stack: u32) -> Self {
        let frame_size = (BASE_FRAME + extra_stack + 15) & !15;
        let mut ops = Assembler::new().expect("failed to create assembler");
        let error_exit = ops.new_dynamic_label();
        let entry = AssemblyOffset(0);

        EmitCtx {
            ops,
            error_exit,
            entry,
            frame_size,
        }
    }

    /// Emit a function prologue. Returns the entry offset and a fresh error_exit label.
    ///
    /// The returned error_exit label must be passed to `end_func` when done emitting
    /// this function's body.
    ///
    /// # Register assignments after prologue
    /// - r12 = cached input_ptr
    /// - r13 = cached input_end
    /// - r14 = out pointer
    /// - r15 = ctx pointer
    pub fn begin_func(&mut self) -> (AssemblyOffset, DynamicLabel) {
        let error_exit = self.ops.new_dynamic_label();
        let entry = self.ops.offset();
        let frame_size = self.frame_size;

        // On entry: rsp is 8-mod-16 (return address was pushed by `call`).
        // push rbp → rsp is now 16-byte aligned.
        // sub rsp, frame_size → stays 16-byte aligned (frame_size is multiple of 16).
        //
        // System V AMD64 frame layout (BASE_FRAME = 48):
        //   [rsp+0]:  saved rbp      [rsp+8]:  (padding)
        //   [rsp+16]: saved r12      [rsp+24]: saved r13
        //   [rsp+32]: saved r14      [rsp+40]: saved r15
        //   [rsp+48..]: extra stack  (args arrive in rdi=out, rsi=ctx)
        //
        // Windows x64 frame layout (BASE_FRAME = 80):
        //   [rsp+0..31]: shadow/home space (32 bytes, callee may write here)
        //   [rsp+32]: saved rbp      [rsp+40]: (padding)
        //   [rsp+48]: saved r12      [rsp+56]: saved r13
        //   [rsp+64]: saved r14      [rsp+72]: saved r15
        //   [rsp+80..]: extra stack  (args arrive in rcx=out, rdx=ctx)
        #[cfg(not(windows))]
        dynasm!(self.ops
            ; .arch x64
            ; push rbp
            ; sub rsp, frame_size as i32
            ; mov [rsp], rbp
            ; mov [rsp + 16], r12
            ; mov [rsp + 24], r13
            ; mov [rsp + 32], r14
            ; mov [rsp + 40], r15
            ; mov r14, rdi              // r14 = out
            ; mov r15, rsi              // r15 = ctx
            ; mov r12, [r15 + CTX_INPUT_PTR as i32]
            ; mov r13, [r15 + CTX_INPUT_END as i32]
        );
        #[cfg(windows)]
        dynasm!(self.ops
            ; .arch x64
            ; push rbp
            ; sub rsp, frame_size as i32
            // [rsp+0..31] is shadow space; callee-saved regs follow after
            ; mov [rsp + 32], rbp
            ; mov [rsp + 48], r12
            ; mov [rsp + 56], r13
            ; mov [rsp + 64], r14
            ; mov [rsp + 72], r15
            ; mov r14, rcx              // r14 = out  (Windows arg0)
            ; mov r15, rdx              // r15 = ctx  (Windows arg1)
            ; mov r12, [r15 + CTX_INPUT_PTR as i32]
            ; mov r13, [r15 + CTX_INPUT_END as i32]
        );

        self.error_exit = error_exit;
        (entry, error_exit)
    }

    /// Emit the success epilogue and error exit for the current function.
    ///
    /// `error_exit` must be the label returned by the corresponding `begin_func` call.
    pub fn end_func(&mut self, error_exit: DynamicLabel) {
        let frame_size = self.frame_size as i32;

        #[cfg(not(windows))]
        dynasm!(self.ops
            ; .arch x64
            // Success path: flush cursor, restore registers, return
            ; mov [r15 + CTX_INPUT_PTR as i32], r12
            ; mov r15, [rsp + 40]
            ; mov r14, [rsp + 32]
            ; mov r13, [rsp + 24]
            ; mov r12, [rsp + 16]
            ; mov rbp, [rsp]
            ; add rsp, frame_size
            ; pop rbp
            ; ret

            // Error exit: just restore and return (error is already in ctx.error)
            ; =>error_exit
            ; mov r15, [rsp + 40]
            ; mov r14, [rsp + 32]
            ; mov r13, [rsp + 24]
            ; mov r12, [rsp + 16]
            ; mov rbp, [rsp]
            ; add rsp, frame_size
            ; pop rbp
            ; ret
        );
        #[cfg(windows)]
        dynasm!(self.ops
            ; .arch x64
            // Success path: flush cursor, restore registers, return
            ; mov [r15 + CTX_INPUT_PTR as i32], r12
            ; mov r15, [rsp + 72]
            ; mov r14, [rsp + 64]
            ; mov r13, [rsp + 56]
            ; mov r12, [rsp + 48]
            ; mov rbp, [rsp + 32]
            ; add rsp, frame_size
            ; pop rbp
            ; ret

            // Error exit: just restore and return (error is already in ctx.error)
            ; =>error_exit
            ; mov r15, [rsp + 72]
            ; mov r14, [rsp + 64]
            ; mov r13, [rsp + 56]
            ; mov r12, [rsp + 48]
            ; mov rbp, [rsp + 32]
            ; add rsp, frame_size
            ; pop rbp
            ; ret
        );
    }

    /// Emit a call to another emitted function.
    ///
    /// Convention: rdi = out + field_offset, rsi = ctx (same as our entry convention).
    /// Flushes cursor before call, reloads after, checks error.
    ///
    /// r[impl callconv.inter-function]
    pub fn emit_call_emitted_func(&mut self, label: DynamicLabel, field_offset: u32) {
        let error_exit = self.error_exit;

        dynasm!(self.ops
            ; .arch x64
            // Flush cached cursor back to ctx
            ; mov [r15 + CTX_INPUT_PTR as i32], r12
        );
        // Set up arguments: arg0 = out + field_offset, arg1 = ctx
        #[cfg(not(windows))]
        dynasm!(self.ops ; .arch x64 ; lea rdi, [r14 + field_offset as i32] ; mov rsi, r15);
        #[cfg(windows)]
        dynasm!(self.ops ; .arch x64 ; lea rcx, [r14 + field_offset as i32] ; mov rdx, r15);
        dynasm!(self.ops
            ; .arch x64
            // Call the emitted function via PC-relative call
            ; call =>label

            // Reload cached cursor from ctx (callee may have advanced it)
            ; mov r12, [r15 + CTX_INPUT_PTR as i32]

            // Check error: if ctx.error.code != 0, branch to error exit
            ; mov r10d, [r15 + CTX_ERROR_CODE as i32]
            ; test r10d, r10d
            ; jnz =>error_exit
        );
    }

    /// Emit a call to an intrinsic function.
    ///
    /// Before the call: flushes the cached input_ptr back to ctx.
    /// Sets up args: rdi = ctx, rsi = out + field_offset.
    /// After the call: reloads input_ptr from ctx, checks error slot.
    pub fn emit_call_intrinsic(&mut self, fn_ptr: *const u8, field_offset: u32) {
        let error_exit = self.error_exit;
        let ptr_val = fn_ptr as i64;

        dynasm!(self.ops
            ; .arch x64
            // Flush cached cursor back to ctx
            ; mov [r15 + CTX_INPUT_PTR as i32], r12
        );
        // Set up arguments: arg0 = ctx, arg1 = out + field_offset
        #[cfg(not(windows))]
        dynasm!(self.ops ; .arch x64 ; mov rdi, r15 ; lea rsi, [r14 + field_offset as i32]);
        #[cfg(windows)]
        dynasm!(self.ops ; .arch x64 ; mov rcx, r15 ; lea rdx, [r14 + field_offset as i32]);
        dynasm!(self.ops
            ; .arch x64
            // Load function pointer into rax and call
            ; mov rax, QWORD ptr_val
            ; call rax

            // Reload cached cursor from ctx (intrinsic may have advanced it)
            ; mov r12, [r15 + CTX_INPUT_PTR as i32]

            // Check error: if ctx.error.code != 0, branch to error exit
            ; mov r10d, [r15 + CTX_ERROR_CODE as i32]
            ; test r10d, r10d
            ; jnz =>error_exit
        );
    }

    /// Emit a call to an intrinsic that takes only ctx as argument.
    /// Flushes cursor, calls, reloads cursor, checks error.
    pub fn emit_call_intrinsic_ctx_only(&mut self, fn_ptr: *const u8) {
        let error_exit = self.error_exit;
        let ptr_val = fn_ptr as i64;

        dynasm!(self.ops ; .arch x64 ; mov [r15 + CTX_INPUT_PTR as i32], r12);
        #[cfg(not(windows))]
        dynasm!(self.ops ; .arch x64 ; mov rdi, r15);
        #[cfg(windows)]
        dynasm!(self.ops ; .arch x64 ; mov rcx, r15);
        dynasm!(self.ops
            ; .arch x64
            ; mov rax, QWORD ptr_val
            ; call rax
            ; mov r12, [r15 + CTX_INPUT_PTR as i32]
            ; mov r10d, [r15 + CTX_ERROR_CODE as i32]
            ; test r10d, r10d
            ; jnz =>error_exit
        );
    }

    /// Emit a call to an intrinsic that takes (ctx, &mut stack_slot).
    /// arg0 = ctx, arg1 = rsp + sp_offset. Flushes/reloads cursor, checks error.
    pub fn emit_call_intrinsic_ctx_and_stack_out(&mut self, fn_ptr: *const u8, sp_offset: u32) {
        let error_exit = self.error_exit;
        let ptr_val = fn_ptr as i64;

        dynasm!(self.ops ; .arch x64 ; mov [r15 + CTX_INPUT_PTR as i32], r12);
        #[cfg(not(windows))]
        dynasm!(self.ops ; .arch x64 ; mov rdi, r15 ; lea rsi, [rsp + sp_offset as i32]);
        #[cfg(windows)]
        dynasm!(self.ops ; .arch x64 ; mov rcx, r15 ; lea rdx, [rsp + sp_offset as i32]);
        dynasm!(self.ops
            ; .arch x64
            ; mov rax, QWORD ptr_val
            ; call rax
            ; mov r12, [r15 + CTX_INPUT_PTR as i32]
            ; mov r10d, [r15 + CTX_ERROR_CODE as i32]
            ; test r10d, r10d
            ; jnz =>error_exit
        );
    }

    /// Emit a call to an intrinsic that takes (ctx, out_ptr1, out_ptr2).
    /// arg0 = ctx, arg1 = rsp + sp_offset1, arg2 = rsp + sp_offset2.
    pub fn emit_call_intrinsic_ctx_and_two_stack_outs(
        &mut self,
        fn_ptr: *const u8,
        sp_offset1: u32,
        sp_offset2: u32,
    ) {
        let error_exit = self.error_exit;
        let ptr_val = fn_ptr as i64;

        dynasm!(self.ops ; .arch x64 ; mov [r15 + CTX_INPUT_PTR as i32], r12);
        #[cfg(not(windows))]
        dynasm!(self.ops ; .arch x64
            ; mov rdi, r15
            ; lea rsi, [rsp + sp_offset1 as i32]
            ; lea rdx, [rsp + sp_offset2 as i32]
        );
        #[cfg(windows)]
        dynasm!(self.ops ; .arch x64
            ; mov rcx, r15
            ; lea rdx, [rsp + sp_offset1 as i32]
            ; lea r8, [rsp + sp_offset2 as i32]
        );
        dynasm!(self.ops
            ; .arch x64
            ; mov rax, QWORD ptr_val
            ; call rax
            ; mov r12, [r15 + CTX_INPUT_PTR as i32]
            ; mov r10d, [r15 + CTX_ERROR_CODE as i32]
            ; test r10d, r10d
            ; jnz =>error_exit
        );
    }

    /// Emit a call to a pure function (no ctx, no flush/reload/error-check).
    /// Used for key_equals: arg0=key_ptr, arg1=key_len, arg2=expected_ptr, arg3=expected_len.
    /// Return value is in rax.
    pub fn emit_call_pure_4arg(
        &mut self,
        fn_ptr: *const u8,
        arg0_sp_offset: u32,
        arg1_sp_offset: u32,
        expected_ptr: *const u8,
        expected_len: u32,
    ) {
        let ptr_val = fn_ptr as i64;
        let expected_addr = expected_ptr as i64;

        #[cfg(not(windows))]
        dynasm!(self.ops
            ; .arch x64
            ; mov rdi, [rsp + arg0_sp_offset as i32]
            ; mov rsi, [rsp + arg1_sp_offset as i32]
            ; mov rdx, QWORD expected_addr
            ; mov ecx, expected_len as i32
            ; mov rax, QWORD ptr_val
            ; call rax
        );
        #[cfg(windows)]
        dynasm!(self.ops
            ; .arch x64
            ; mov rcx, [rsp + arg0_sp_offset as i32]
            ; mov rdx, [rsp + arg1_sp_offset as i32]
            ; mov r8, QWORD expected_addr
            ; mov r9d, expected_len as i32
            ; mov rax, QWORD ptr_val
            ; call rax
        );
    }

    /// Allocate a new dynamic label.
    pub fn new_label(&mut self) -> DynamicLabel {
        self.ops.new_dynamic_label()
    }

    /// Bind a dynamic label at the current position.
    pub fn bind_label(&mut self, label: DynamicLabel) {
        dynasm!(self.ops
            ; .arch x64
            ; =>label
        );
    }

    /// Emit an unconditional branch to the given label.
    pub fn emit_branch(&mut self, label: DynamicLabel) {
        dynasm!(self.ops
            ; .arch x64
            ; jmp =>label
        );
    }

    /// Emit `test rax, rax; jnz label` — branch if rax is nonzero.
    pub fn emit_cbnz_x0(&mut self, label: DynamicLabel) {
        dynasm!(self.ops
            ; .arch x64
            ; test rax, rax
            ; jnz =>label
        );
    }

    /// Zero a 64-bit stack slot at rsp + offset.
    pub fn emit_zero_stack_slot(&mut self, sp_offset: u32) {
        dynasm!(self.ops
            ; .arch x64
            ; mov QWORD [rsp + sp_offset as i32], 0
        );
    }

    /// Load a byte from rsp + sp_offset, compare with byte_val, branch if equal.
    pub fn emit_stack_byte_cmp_branch(&mut self, sp_offset: u32, byte_val: u8, label: DynamicLabel) {
        let byte_val = byte_val as i32;
        dynasm!(self.ops
            ; .arch x64
            ; movzx r10d, BYTE [rsp + sp_offset as i32]
            ; cmp r10d, byte_val
            ; je =>label
        );
    }

    /// Set bit `bit_index` in a 64-bit stack slot at rsp + sp_offset.
    pub fn emit_set_bit_on_stack(&mut self, sp_offset: u32, bit_index: u32) {
        let mask = (1u64 << bit_index) as i64;

        dynasm!(self.ops
            ; .arch x64
            ; mov rax, [rsp + sp_offset as i32]
            ; mov r10, QWORD mask
            ; or rax, r10
            ; mov [rsp + sp_offset as i32], rax
        );
    }

    /// Check that the 64-bit stack slot at rsp + sp_offset equals expected_mask.
    /// If not, set MissingRequiredField error and branch to error_exit.
    pub fn emit_check_bitset(&mut self, sp_offset: u32, expected_mask: u64) {
        let error_exit = self.error_exit;
        let ok_label = self.ops.new_dynamic_label();
        let expected_mask = expected_mask as i64;
        let error_code = crate::context::ErrorCode::MissingRequiredField as i32;

        dynasm!(self.ops
            ; .arch x64
            ; mov rax, [rsp + sp_offset as i32]
            ; mov r10, QWORD expected_mask
            ; cmp rax, r10
            ; je =>ok_label
            // Not all required fields were seen — write error and bail
            ; mov DWORD [r15 + CTX_ERROR_CODE as i32], error_code
            ; jmp =>error_exit
            ; =>ok_label
        );
    }

    // ── Inline scalar reads (recipe-based) ─────────────────────────────

    pub fn emit_inline_read_byte(&mut self, offset: u32) {
        self.emit_recipe(&crate::recipe::read_byte(offset));
    }

    pub fn emit_inline_read_byte_to_stack(&mut self, sp_offset: u32) {
        self.emit_recipe(&crate::recipe::read_byte_to_stack(sp_offset));
    }

    pub fn emit_inline_read_bool(&mut self, offset: u32) {
        self.emit_recipe(&crate::recipe::read_bool(offset));
    }

    pub fn emit_inline_read_f32(&mut self, offset: u32) {
        self.emit_recipe(&crate::recipe::read_f32(offset));
    }

    pub fn emit_inline_read_f64(&mut self, offset: u32) {
        self.emit_recipe(&crate::recipe::read_f64(offset));
    }

    pub fn emit_inline_varint_fast_path(
        &mut self,
        offset: u32,
        store_width: u32,
        zigzag: bool,
        intrinsic_fn_ptr: *const u8,
    ) {
        let width = match store_width {
            2 => Width::W2,
            4 => Width::W4,
            8 => Width::W8,
            _ => panic!("unsupported varint store width: {store_width}"),
        };
        self.emit_recipe(&crate::recipe::varint_fast_path(offset, width, zigzag, intrinsic_fn_ptr));
    }

    // ── Inline string reads (recipe-based) ──────────────────────────────

    pub fn emit_inline_postcard_string_malum(
        &mut self,
        offset: u32,
        string_offsets: &crate::malum::StringOffsets,
        slow_varint_intrinsic: *const u8,
        validate_alloc_copy_intrinsic: *const u8,
    ) {
        self.emit_recipe(&crate::recipe::postcard_string_malum(
            offset,
            string_offsets,
            slow_varint_intrinsic,
            validate_alloc_copy_intrinsic,
        ));
    }

    // ── Enum support ──────────────────────────────────────────────────

    // r[impl deser.enum.set-variant]

    /// Write a discriminant value to [out + 0].
    /// `size` is 1, 2, 4, or 8 bytes (from EnumRepr).
    pub fn emit_write_discriminant(&mut self, value: i64, size: u32) {
        match size {
            1 => dynasm!(self.ops ; .arch x64
                ; mov BYTE [r14], value as i8
            ),
            2 => dynasm!(self.ops ; .arch x64
                ; mov WORD [r14], value as i16
            ),
            4 => dynasm!(self.ops ; .arch x64
                ; mov DWORD [r14], value as i32
            ),
            8 => {
                let val = value;
                dynasm!(self.ops ; .arch x64
                    ; mov rax, QWORD val
                    ; mov QWORD [r14], rax
                );
            }
            _ => panic!("unsupported discriminant size: {size}"),
        }
    }

    // r[impl deser.postcard.enum.dispatch]

    /// Read a postcard varint discriminant into r10d (kept in register for
    /// dispatch, not stored to memory).
    ///
    /// After this, the caller emits `emit_cmp_imm_branch_eq` for each variant.
    /// The discriminant value is in r10d.
    pub fn emit_read_postcard_discriminant(&mut self, slow_intrinsic: *const u8) {
        let error_exit = self.error_exit;
        let eof_label = self.ops.new_dynamic_label();
        let slow_path = self.ops.new_dynamic_label();
        let done_label = self.ops.new_dynamic_label();
        let ptr_val = slow_intrinsic as i64;

        dynasm!(self.ops
            ; .arch x64
            // Bounds check
            ; cmp r12, r13
            ; jae =>eof_label
            // Load first byte
            ; movzx r10d, BYTE [r12]
            // Test continuation bit
            ; test r10d, 0x80
            ; jnz =>slow_path
            // Fast path: single-byte varint
            ; add r12, 1
            ; jmp =>done_label

            // Slow path: call full varint decode intrinsic
            ; =>slow_path
            ; mov [r15 + CTX_INPUT_PTR as i32], r12
        );
        // arg0 = ctx, arg1 = pointer to temp u32 at BASE_FRAME (start of extra area)
        #[cfg(not(windows))]
        dynasm!(self.ops ; .arch x64 ; mov rdi, r15 ; lea rsi, [rsp + BASE_FRAME as i32]);
        #[cfg(windows)]
        dynasm!(self.ops ; .arch x64 ; mov rcx, r15 ; lea rdx, [rsp + BASE_FRAME as i32]);
        dynasm!(self.ops
            ; .arch x64
            ; mov rax, QWORD ptr_val
            ; call rax
            ; mov r12, [r15 + CTX_INPUT_PTR as i32]
            ; mov r11d, [r15 + CTX_ERROR_CODE as i32]
            ; test r11d, r11d
            ; jnz =>error_exit
            ; mov r10d, DWORD [rsp + BASE_FRAME as i32]  // load decoded discriminant
            ; jmp =>done_label

            ; =>eof_label
            ; mov DWORD [r15 + CTX_ERROR_CODE as i32], crate::context::ErrorCode::UnexpectedEof as i32
            ; jmp =>error_exit

            ; =>done_label
        );
    }

    /// Compare r10d (discriminant) with immediate `imm` and branch to `label`
    /// if equal.
    pub fn emit_cmp_imm_branch_eq(&mut self, imm: u32, label: DynamicLabel) {
        dynasm!(self.ops
            ; .arch x64
            ; cmp r10d, imm as i32
            ; je =>label
        );
    }

    /// Emit a branch-to-error for unknown variant (sets UnknownVariant error code).
    pub fn emit_unknown_variant_error(&mut self) {
        let error_exit = self.error_exit;
        let error_code = crate::context::ErrorCode::UnknownVariant as i32;

        dynasm!(self.ops
            ; .arch x64
            ; mov DWORD [r15 + CTX_ERROR_CODE as i32], error_code
            ; jmp =>error_exit
        );
    }

    /// Save the cached input_ptr (r12) to a stack slot.
    pub fn emit_save_input_ptr(&mut self, stack_offset: u32) {
        dynasm!(self.ops
            ; .arch x64
            ; mov [rsp + stack_offset as i32], r12
        );
    }

    /// Restore the cached input_ptr (r12) from a stack slot.
    pub fn emit_restore_input_ptr(&mut self, stack_offset: u32) {
        dynasm!(self.ops
            ; .arch x64
            ; mov r12, [rsp + stack_offset as i32]
        );
    }

    /// Store a 64-bit immediate into a stack slot at rsp + offset.
    pub fn emit_store_imm64_to_stack(&mut self, stack_offset: u32, value: u64) {
        let val = value as i64;
        dynasm!(self.ops
            ; .arch x64
            ; mov rax, QWORD val
            ; mov [rsp + stack_offset as i32], rax
        );
    }

    /// AND a 64-bit immediate into a stack slot at rsp + offset.
    /// Loads the slot, ANDs with the immediate, stores back.
    pub fn emit_and_imm64_on_stack(&mut self, stack_offset: u32, mask: u64) {
        let mask_val = mask as i64;
        dynasm!(self.ops
            ; .arch x64
            ; mov rax, [rsp + stack_offset as i32]
            ; mov r10, QWORD mask_val
            ; and rax, r10
            ; mov [rsp + stack_offset as i32], rax
        );
    }

    /// Check if the stack slot at rsp + offset has exactly one bit set (popcount == 1).
    /// If so, branch to `label`.
    pub fn emit_popcount_eq1_branch(&mut self, stack_offset: u32, label: DynamicLabel) {
        dynasm!(self.ops
            ; .arch x64
            ; mov rax, [rsp + stack_offset as i32]
            // popcount == 1 iff (x & (x-1)) == 0 && x != 0
            ; lea r10, [rax - 1]
            ; test rax, r10        // sets Z if (x & (x-1)) == 0
            ; jnz >skip            // more than 1 bit
            ; test rax, rax        // check nonzero
            ; jnz =>label          // exactly 1 bit
            ; skip:
        );
    }

    /// Check if the stack slot at rsp + offset is zero. If so, branch to `label`.
    pub fn emit_stack_zero_branch(&mut self, stack_offset: u32, label: DynamicLabel) {
        dynasm!(self.ops
            ; .arch x64
            ; mov rax, [rsp + stack_offset as i32]
            ; test rax, rax
            ; jz =>label
        );
    }

    /// Load the stack slot at rsp + offset, test if bit `bit_index` is set,
    /// and branch to `label` if so.
    pub fn emit_test_bit_branch(&mut self, stack_offset: u32, bit_index: u32, label: DynamicLabel) {
        let mask = (1u64 << bit_index) as i64;
        dynasm!(self.ops
            ; .arch x64
            ; mov rax, [rsp + stack_offset as i32]
            ; mov r10, QWORD mask
            ; test rax, r10
            ; jnz =>label
        );
    }

    /// Emit an error (write error code to ctx, jump to error_exit).
    pub fn emit_error(&mut self, code: crate::context::ErrorCode) {
        let error_exit = self.error_exit;
        let error_code = code as i32;
        dynasm!(self.ops
            ; .arch x64
            ; mov DWORD [r15 + CTX_ERROR_CODE as i32], error_code
            ; jmp =>error_exit
        );
    }

    /// Advance the cached cursor by n bytes (inline, no function call).
    pub fn emit_advance_cursor_by(&mut self, n: u32) {
        dynasm!(self.ops
            ; .arch x64
            ; add r12, n as i32
        );
    }

    // ── Option support ────────────────────────────────────────────────

    /// Save the current `out` pointer (r14) and redirect it to a stack scratch area.
    pub fn emit_redirect_out_to_stack(&mut self, scratch_offset: u32) {
        dynasm!(self.ops
            ; .arch x64
            ; mov [rsp + (scratch_offset as i32 - 8)], r14
            ; lea r14, [rsp + scratch_offset as i32]
        );
    }

    /// Restore the `out` pointer (r14) from the saved slot.
    pub fn emit_restore_out(&mut self, scratch_offset: u32) {
        dynasm!(self.ops
            ; .arch x64
            ; mov r14, [rsp + (scratch_offset as i32 - 8)]
        );
    }

    /// Call fad_option_init_none(init_none_fn, out + offset).
    /// Does not touch ctx or the cursor.
    pub fn emit_call_option_init_none(&mut self, wrapper_fn: *const u8, init_none_fn: *const u8, offset: u32) {
        let wrapper_val = wrapper_fn as i64;
        let init_none_val = init_none_fn as i64;

        // arg0: init_none_fn pointer, arg1: out + offset
        #[cfg(not(windows))]
        dynasm!(self.ops ; .arch x64
            ; mov rdi, QWORD init_none_val
            ; lea rsi, [r14 + offset as i32]
        );
        #[cfg(windows)]
        dynasm!(self.ops ; .arch x64
            ; mov rcx, QWORD init_none_val
            ; lea rdx, [r14 + offset as i32]
        );
        dynasm!(self.ops ; .arch x64 ; mov rax, QWORD wrapper_val ; call rax);
    }

    /// Call fad_option_init_some(init_some_fn, out + offset, sp + scratch_offset).
    /// Does not touch ctx or the cursor.
    pub fn emit_call_option_init_some(
        &mut self,
        wrapper_fn: *const u8,
        init_some_fn: *const u8,
        offset: u32,
        scratch_offset: u32,
    ) {
        let wrapper_val = wrapper_fn as i64;
        let init_some_val = init_some_fn as i64;

        // arg0: init_some_fn pointer, arg1: out + offset, arg2: scratch area on stack
        #[cfg(not(windows))]
        dynasm!(self.ops ; .arch x64
            ; mov rdi, QWORD init_some_val
            ; lea rsi, [r14 + offset as i32]
            ; lea rdx, [rsp + scratch_offset as i32]
        );
        #[cfg(windows)]
        dynasm!(self.ops ; .arch x64
            ; mov rcx, QWORD init_some_val
            ; lea rdx, [r14 + offset as i32]
            ; lea r8, [rsp + scratch_offset as i32]
        );
        dynasm!(self.ops ; .arch x64 ; mov rax, QWORD wrapper_val ; call rax);
    }

    // ── Vec support ──────────────────────────────────────────────────

    /// Call fad_vec_alloc with a constant count (for JSON initial allocation).
    ///
    /// Result (buf pointer) is in rax.
    pub fn emit_call_json_vec_initial_alloc(
        &mut self,
        alloc_fn: *const u8,
        count: u32,
        elem_size: u32,
        elem_align: u32,
    ) {
        let error_exit = self.error_exit;
        let ptr_val = alloc_fn as i64;

        dynasm!(self.ops ; .arch x64 ; mov [r15 + CTX_INPUT_PTR as i32], r12);
        // args: ctx, count, elem_size, elem_align
        #[cfg(not(windows))]
        dynasm!(self.ops ; .arch x64
            ; mov rdi, r15
            ; mov esi, count as i32
            ; mov edx, elem_size as i32
            ; mov ecx, elem_align as i32
        );
        #[cfg(windows)]
        dynasm!(self.ops ; .arch x64
            ; mov rcx, r15
            ; mov edx, count as i32
            ; mov r8d, elem_size as i32
            ; mov r9d, elem_align as i32
        );
        dynasm!(self.ops
            ; .arch x64
            ; mov rax, QWORD ptr_val
            ; call rax
            ; mov r12, [r15 + CTX_INPUT_PTR as i32]
            ; mov r10d, [r15 + CTX_ERROR_CODE as i32]
            ; test r10d, r10d
            ; jnz =>error_exit
        );
    }

    /// Initialize JSON Vec loop state: buf from rax, len=0, cap=initial_cap.
    pub fn emit_json_vec_loop_init(
        &mut self,
        saved_out_slot: u32,
        buf_slot: u32,
        len_slot: u32,
        cap_slot: u32,
        initial_cap: u32,
    ) {
        dynasm!(self.ops
            ; .arch x64
            ; mov [rsp + saved_out_slot as i32], r14  // save out pointer
            ; mov [rsp + buf_slot as i32], rax         // buf = alloc result
            ; mov QWORD [rsp + len_slot as i32], 0     // len = 0
            ; mov DWORD [rsp + cap_slot as i32], initial_cap as i32
            ; mov DWORD [rsp + (cap_slot as i32 + 4)], 0  // zero upper 32 bits
        );
    }

    /// Check ctx.error.code and branch to label if nonzero.
    pub fn emit_check_error_branch(&mut self, label: DynamicLabel) {
        dynasm!(self.ops
            ; .arch x64
            ; mov r10d, [r15 + CTX_ERROR_CODE as i32]
            ; test r10d, r10d
            ; jnz =>label
        );
    }

    /// Save the count register (w9 on aarch64, r10d on x64) to a stack slot.
    ///
    /// Used to preserve the count across a function call (r10 is caller-saved).
    pub fn emit_save_count_to_stack(&mut self, slot: u32) {
        dynasm!(self.ops
            ; .arch x64
            ; mov [rsp + slot as i32], r10
        );
    }

    /// Call fad_vec_alloc(ctx, count, elem_size, elem_align).
    ///
    /// count is in r10d (from emit_read_postcard_discriminant or JSON parse).
    /// Result (buf pointer) is in rax.
    ///
    /// **Important**: r10 is caller-saved and will be clobbered by the call.
    /// The count must be saved to a stack slot before calling this.
    pub fn emit_call_vec_alloc(
        &mut self,
        alloc_fn: *const u8,
        elem_size: u32,
        elem_align: u32,
    ) {
        let error_exit = self.error_exit;
        let ptr_val = alloc_fn as i64;

        dynasm!(self.ops ; .arch x64 ; mov [r15 + CTX_INPUT_PTR as i32], r12);
        // args: ctx, count(r10), elem_size, elem_align
        #[cfg(not(windows))]
        dynasm!(self.ops ; .arch x64
            ; mov rdi, r15
            ; mov rsi, r10
            ; mov edx, elem_size as i32
            ; mov ecx, elem_align as i32
        );
        #[cfg(windows)]
        dynasm!(self.ops ; .arch x64
            ; mov rcx, r15
            ; mov rdx, r10
            ; mov r8d, elem_size as i32
            ; mov r9d, elem_align as i32
        );
        dynasm!(self.ops
            ; .arch x64
            ; mov rax, QWORD ptr_val
            ; call rax
            ; mov r12, [r15 + CTX_INPUT_PTR as i32]
            ; mov r10d, [r15 + CTX_ERROR_CODE as i32]
            ; test r10d, r10d
            ; jnz =>error_exit
        );
    }

    /// Call fad_vec_grow(ctx, old_buf, len, old_cap, new_cap, elem_size, elem_align).
    ///
    /// Reads old_buf, len, old_cap from stack slots. Computes new_cap = old_cap * 2.
    /// After call: new buf pointer is in rax.
    pub fn emit_call_vec_grow(
        &mut self,
        grow_fn: *const u8,
        buf_slot: u32,
        len_slot: u32,
        cap_slot: u32,
        elem_size: u32,
        elem_align: u32,
    ) {
        let error_exit = self.error_exit;
        let ptr_val = grow_fn as i64;

        dynasm!(self.ops ; .arch x64 ; mov [r15 + CTX_INPUT_PTR as i32], r12);

        #[cfg(not(windows))]
        {
            // System V AMD64: 6 register args + 7th on the stack via push.
            // Args: rdi=ctx, rsi=old_buf, rdx=len, rcx=old_cap, r8=new_cap, r9=elem_size
            // 7th arg (elem_align) pushed before call.
            dynasm!(self.ops
                ; .arch x64
                ; mov r10, [rsp + cap_slot as i32]
                ; shl r10, 1                         // new_cap = old_cap * 2
                ; mov rdi, r15
                ; mov rsi, [rsp + buf_slot as i32]
                ; mov rdx, [rsp + len_slot as i32]
                ; mov rcx, [rsp + cap_slot as i32]
                ; mov r8, r10
                ; mov r9d, elem_size as i32
                ; push elem_align as i32
                ; mov rax, QWORD ptr_val
                ; call rax
                ; add rsp, 8
            );
        }

        #[cfg(windows)]
        {
            // Windows x64: 4 register args + args 5-7 on the stack.
            // The shadow space (32 bytes) and 3 stack args (24 bytes) = 56 bytes.
            // Round up to 64 for 16-byte alignment before `call`.
            //
            // Load all values into registers BEFORE sub rsp (frame offsets change after).
            // Args: rcx=ctx, rdx=old_buf, r8=len, r9=old_cap
            // Stack: [rsp+32]=new_cap, [rsp+40]=elem_size, [rsp+48]=elem_align
            dynasm!(self.ops
                ; .arch x64
                ; mov r10, [rsp + cap_slot as i32]
                ; shl r10, 1                         // r10 = new_cap = old_cap * 2
                ; mov rcx, r15
                ; mov rdx, [rsp + buf_slot as i32]
                ; mov r8, [rsp + len_slot as i32]
                ; mov r9, [rsp + cap_slot as i32]
                ; sub rsp, 64                        // 32 shadow + 24 args + 8 padding
                ; mov [rsp + 32], r10                // arg5 = new_cap (64-bit)
                // arg6 and arg7: use mov eax,N to zero-extend to 64 bits, then store
                ; mov eax, elem_size as i32
                ; mov [rsp + 40], rax                // arg6 = elem_size (zero-extended)
                ; mov eax, elem_align as i32
                ; mov [rsp + 48], rax                // arg7 = elem_align (zero-extended)
                ; mov rax, QWORD ptr_val
                ; call rax
                ; add rsp, 64
            );
        }

        dynasm!(self.ops
            ; .arch x64
            ; mov r12, [r15 + CTX_INPUT_PTR as i32]
            ; mov r10d, [r15 + CTX_ERROR_CODE as i32]
            ; test r10d, r10d
            ; jnz =>error_exit
            ; mov [rsp + buf_slot as i32], rax
            ; mov r10, [rsp + cap_slot as i32]
            ; shl r10, 1
            ; mov [rsp + cap_slot as i32], r10
        );
    }

    /// Initialize Vec loop state after allocation.
    ///
    /// Saves: out pointer, buf (alloc result in rax), counter=0.
    /// Count must already be stored at count_slot (via emit_save_r10_to_stack).
    /// Initialize Vec loop state with cursor in rbx (callee-saved), end on stack.
    ///
    /// Saves rbx to stack, then sets rbx = buf (cursor), end_slot = buf + count * elem_size.
    /// Also saves out pointer and buf for final Vec store.
    pub fn emit_vec_loop_init_cursor(
        &mut self,
        saved_out_slot: u32,
        buf_slot: u32,
        count_slot: u32,
        save_rbx_slot: u32,
        end_slot: u32,
        elem_size: u32,
    ) {
        dynasm!(self.ops
            ; .arch x64
            ; mov [rsp + saved_out_slot as i32], r14  // save out pointer
            ; mov [rsp + buf_slot as i32], rax        // buf = alloc result
            // Save callee-saved rbx
            ; mov [rsp + save_rbx_slot as i32], rbx
            // rbx = cursor = buf
            ; mov rbx, rax
            // end = buf + count * elem_size
            ; mov r10, [rsp + count_slot as i32]
            ; imul r10, r10, elem_size as i32
            ; add r10, rax
            ; mov [rsp + end_slot as i32], r10
        );
    }

    /// Set out = cursor (r14 = rbx). Single register move.
    pub fn emit_vec_loop_load_cursor(&mut self, _save_rbx_slot: u32) {
        dynasm!(self.ops
            ; .arch x64
            ; mov r14, rbx
        );
    }

    /// Advance cursor register, check error, branch back if cursor < end.
    /// Cursor advance is register-only; end compare reads from stack (x64 can cmp reg, [mem]).
    pub fn emit_vec_loop_advance_cursor(
        &mut self,
        _save_rbx_slot: u32,
        end_slot: u32,
        elem_size: u32,
        loop_label: DynamicLabel,
        error_cleanup_label: DynamicLabel,
    ) {
        dynasm!(self.ops
            ; .arch x64
            // Check error from element deserialization
            ; mov r10d, [r15 + CTX_ERROR_CODE as i32]
            ; test r10d, r10d
            ; jnz =>error_cleanup_label
            // cursor += elem_size (register only)
            ; add rbx, elem_size as i32
            // Compare with end (cmp reg, [mem] — single instruction on x64)
            ; cmp rbx, [rsp + end_slot as i32]
            ; jb =>loop_label
        );
    }

    /// Advance the cursor register and loop back, without checking the error flag.
    ///
    /// Use this when all error paths within the loop body branch directly to
    /// the error cleanup label (e.g. via redirected `error_exit`), making
    /// the per-iteration error check redundant.
    pub fn emit_vec_loop_advance_no_error_check(
        &mut self,
        end_slot: u32,
        elem_size: u32,
        loop_label: DynamicLabel,
    ) {
        dynasm!(self.ops
            ; .arch x64
            ; add rbx, elem_size as i32
            ; cmp rbx, [rsp + end_slot as i32]
            ; jb =>loop_label
        );
    }

    /// Emit a tight varint Vec loop body for x64. Writes directly to `[rbx]`
    /// (the cursor register). The slow path and EOF are placed out-of-line.
    pub fn emit_vec_varint_loop(
        &mut self,
        store_width: u32,
        zigzag: bool,
        intrinsic_fn_ptr: *const u8,
        elem_size: u32,
        end_slot: u32,
        loop_label: DynamicLabel,
        done_label: DynamicLabel,
        error_cleanup: DynamicLabel,
    ) {
        let slow_path = self.ops.new_dynamic_label();
        let eof_label = self.ops.new_dynamic_label();
        let ptr_val = intrinsic_fn_ptr as i64;

        // === Hot loop ===
        dynasm!(self.ops
            ; .arch x64
            ; cmp r12, r13
            ; jae =>eof_label
            ; movzx r10d, BYTE [r12]
            ; add r12, 1
            ; test r10d, 0x80
            ; jnz =>slow_path
        );

        if zigzag {
            dynasm!(self.ops
                ; .arch x64
                ; mov r11d, r10d
                ; shr r11d, 1
                ; and r10d, 1
                ; neg r10d
                ; xor r10d, r11d
            );
        }

        // Store directly to cursor (rbx)
        match store_width {
            2 => dynasm!(self.ops ; .arch x64 ; mov WORD [rbx], r10w),
            4 => dynasm!(self.ops ; .arch x64 ; mov DWORD [rbx], r10d),
            8 => dynasm!(self.ops ; .arch x64 ; mov QWORD [rbx], r10),
            _ => panic!("unsupported varint store width: {store_width}"),
        }

        // Advance cursor, loop back
        dynasm!(self.ops
            ; .arch x64
            ; add rbx, elem_size as i32
            ; cmp rbx, [rsp + end_slot as i32]
            ; jb =>loop_label
            ; jmp =>done_label
        );

        // === Slow path (out-of-line) ===
        dynasm!(self.ops
            ; .arch x64
            ; =>slow_path
            // Undo the add r12, 1 (intrinsic expects r12 at varint start)
            ; sub r12, 1
            // Flush input pointer to ctx
            ; mov [r15 + CTX_INPUT_PTR as i32], r12
        );
        // arg0 = ctx, arg1 = out (cursor in rbx)
        #[cfg(not(windows))]
        dynasm!(self.ops ; .arch x64 ; mov rdi, r15 ; mov rsi, rbx);
        #[cfg(windows)]
        dynasm!(self.ops ; .arch x64 ; mov rcx, r15 ; mov rdx, rbx);
        dynasm!(self.ops
            ; .arch x64
            ; mov rax, QWORD ptr_val
            ; call rax
            // Reload input pointer
            ; mov r12, [r15 + CTX_INPUT_PTR as i32]
            // Check error
            ; mov r10d, [r15 + CTX_ERROR_CODE as i32]
            ; test r10d, r10d
            ; jnz =>error_cleanup
            // Advance cursor, loop back
            ; add rbx, elem_size as i32
            ; cmp rbx, [rsp + end_slot as i32]
            ; jb =>loop_label
            ; jmp =>done_label
        );

        // === EOF (cold) ===
        dynasm!(self.ops
            ; .arch x64
            ; =>eof_label
            ; mov DWORD [r15 + CTX_ERROR_CODE as i32], crate::context::ErrorCode::UnexpectedEof as i32
            ; jmp =>error_cleanup
        );
    }

    /// Restore rbx from stack. Must be called on every exit path from a Vec loop.
    pub fn emit_vec_restore_callee_saved(&mut self, save_rbx_slot: u32, _end_slot: u32) {
        dynasm!(self.ops
            ; .arch x64
            ; mov rbx, [rsp + save_rbx_slot as i32]
        );
    }

    /// Emit Vec loop header: compute slot = buf + i * elem_size, set out = slot.
    ///
    /// Used by JSON where buf can change on growth and index-based access is needed.
    pub fn emit_vec_loop_slot(&mut self, buf_slot: u32, counter_slot: u32, elem_size: u32) {
        dynasm!(self.ops
            ; .arch x64
            ; mov r14, [rsp + buf_slot as i32]         // buf
            ; mov r10, [rsp + counter_slot as i32]     // i
            ; imul r10, r10, elem_size as i32          // i * elem_size
            ; add r14, r10                              // out = buf + i * elem_size
        );
    }

    /// Write Vec fields (ptr, len, cap) to out + base_offset using discovered offsets.
    /// Reads buf and count from stack slots.
    pub fn emit_vec_store(
        &mut self,
        base_offset: u32,
        saved_out_slot: u32,
        buf_slot: u32,
        len_slot: u32,
        cap_slot: u32,
        offsets: &crate::malum::VecOffsets,
    ) {
        let ptr_off = (base_offset + offsets.ptr_offset) as i32;
        let len_off = (base_offset + offsets.len_offset) as i32;
        let cap_off = (base_offset + offsets.cap_offset) as i32;

        dynasm!(self.ops
            ; .arch x64
            // Restore out pointer
            ; mov r14, [rsp + saved_out_slot as i32]
            // Load values and store at discovered offsets
            ; mov rax, [rsp + buf_slot as i32]
            ; mov [r14 + ptr_off], rax
            ; mov rax, [rsp + len_slot as i32]
            ; mov [r14 + len_off], rax
            ; mov rax, [rsp + cap_slot as i32]
            ; mov [r14 + cap_off], rax
        );
    }

    /// Write an empty Vec to out + base_offset with proper dangling pointer.
    pub fn emit_vec_store_empty_with_align(
        &mut self,
        base_offset: u32,
        elem_align: u32,
        offsets: &crate::malum::VecOffsets,
    ) {
        let ptr_off = (base_offset + offsets.ptr_offset) as i32;
        let len_off = (base_offset + offsets.len_offset) as i32;
        let cap_off = (base_offset + offsets.cap_offset) as i32;

        // Vec::new() writes: ptr = NonNull::dangling() = elem_align as *mut T, len = 0, cap = 0.
        dynasm!(self.ops
            ; .arch x64
            ; mov QWORD [r14 + ptr_off], elem_align as i32
            ; mov QWORD [r14 + len_off], 0
            ; mov QWORD [r14 + cap_off], 0
        );
    }

    /// Emit error cleanup for Vec: free the buffer and branch to error exit.
    /// Called when element deserialization fails mid-loop.
    pub fn emit_vec_error_cleanup(
        &mut self,
        free_fn: *const u8,
        saved_out_slot: u32,
        buf_slot: u32,
        cap_slot: u32,
        elem_size: u32,
        elem_align: u32,
    ) {
        let error_exit = self.error_exit;
        let ptr_val = free_fn as i64;

        dynasm!(self.ops ; .arch x64 ; mov r14, [rsp + saved_out_slot as i32]);
        // args: buf, cap, elem_size, elem_align
        #[cfg(not(windows))]
        dynasm!(self.ops ; .arch x64
            ; mov rdi, [rsp + buf_slot as i32]
            ; mov rsi, [rsp + cap_slot as i32]
            ; mov edx, elem_size as i32
            ; mov ecx, elem_align as i32
        );
        #[cfg(windows)]
        dynasm!(self.ops ; .arch x64
            ; mov rcx, [rsp + buf_slot as i32]
            ; mov rdx, [rsp + cap_slot as i32]
            ; mov r8d, elem_size as i32
            ; mov r9d, elem_align as i32
        );
        dynasm!(self.ops ; .arch x64 ; mov rax, QWORD ptr_val ; call rax ; jmp =>error_exit);
    }

    /// Compare the count register (w9 on aarch64, r10d on x64) with zero
    /// and branch to label if equal.
    pub fn emit_cbz_count(&mut self, label: DynamicLabel) {
        dynasm!(self.ops
            ; .arch x64
            ; test r10d, r10d
            ; jz =>label
        );
    }

    /// Compare two stack slot values and branch if equal (len == cap for growth check).
    pub fn emit_cmp_stack_slots_branch_eq(
        &mut self,
        slot_a: u32,
        slot_b: u32,
        label: DynamicLabel,
    ) {
        dynasm!(self.ops
            ; .arch x64
            ; mov rax, [rsp + slot_a as i32]
            ; cmp rax, [rsp + slot_b as i32]
            ; je =>label
        );
    }

    /// Increment a stack slot value by 1.
    pub fn emit_inc_stack_slot(&mut self, slot: u32) {
        dynasm!(self.ops
            ; .arch x64
            ; mov rax, [rsp + slot as i32]
            ; add rax, 1
            ; mov [rsp + slot as i32], rax
        );
    }

    // ── Map support ─────────────────────────────────────────────────

    /// Advance the out register (r14) by a constant offset.
    ///
    /// Used in map loops to move from the key slot to the value slot within a pair.
    pub fn emit_advance_out_by(&mut self, offset: u32) {
        dynasm!(self.ops
            ; .arch x64
            ; add r14, offset as i32
        );
    }

    /// Call `fad_map_build(from_pair_slice_fn, saved_out, pairs_buf, count)`.
    ///
    /// We cannot call `from_pair_slice` directly from JIT code because its
    /// first arg `PtrUninit` is a 16-byte struct — passed in 2 registers on
    /// Linux x64 but by hidden pointer on Windows x64.  `fad_map_build` is a
    /// plain-C trampoline that takes four pointer-/usize-sized args and
    /// constructs `PtrUninit` internally.
    pub fn emit_call_map_from_pairs(
        &mut self,
        from_pair_slice_fn: *const u8,
        saved_out_slot: u32,
        buf_slot: u32,
        count_slot: u32,
    ) {
        let fn_val = from_pair_slice_fn as i64;
        let trampoline = crate::intrinsics::fad_map_build as i64;

        #[cfg(not(windows))]
        dynasm!(self.ops ; .arch x64
            ; mov rdi, QWORD fn_val                    // arg0 = from_pair_slice_fn
            ; mov rsi, [rsp + saved_out_slot as i32]   // arg1 = map_ptr
            ; mov rdx, [rsp + buf_slot as i32]         // arg2 = pairs_ptr
            ; mov rcx, [rsp + count_slot as i32]       // arg3 = count
        );
        #[cfg(windows)]
        dynasm!(self.ops ; .arch x64
            ; mov rcx, QWORD fn_val                    // arg0 = from_pair_slice_fn
            ; mov rdx, [rsp + saved_out_slot as i32]   // arg1 = map_ptr
            ; mov r8, [rsp + buf_slot as i32]          // arg2 = pairs_ptr
            ; mov r9, [rsp + count_slot as i32]        // arg3 = count
        );
        dynasm!(self.ops ; .arch x64 ; mov rax, QWORD trampoline ; call rax);
    }

    /// Call `fad_map_build(from_pair_slice_fn, r14, null, 0)` — empty map.
    ///
    /// Same trampoline pattern as `emit_call_map_from_pairs`.
    pub fn emit_call_map_from_pairs_empty(&mut self, from_pair_slice_fn: *const u8) {
        let fn_val = from_pair_slice_fn as i64;
        let trampoline = crate::intrinsics::fad_map_build as i64;

        #[cfg(not(windows))]
        dynasm!(self.ops ; .arch x64
            ; mov rdi, QWORD fn_val  // arg0 = from_pair_slice_fn
            ; mov rsi, r14           // arg1 = map_ptr (current out)
            ; xor edx, edx           // arg2 = null pairs ptr
            ; xor ecx, ecx           // arg3 = count = 0
        );
        #[cfg(windows)]
        dynasm!(self.ops ; .arch x64
            ; mov rcx, QWORD fn_val  // arg0 = from_pair_slice_fn
            ; mov rdx, r14           // arg1 = map_ptr (current out)
            ; xor r8d, r8d           // arg2 = null pairs ptr
            ; xor r9d, r9d           // arg3 = count = 0
        );
        dynasm!(self.ops ; .arch x64 ; mov rax, QWORD trampoline ; call rax);
    }

    /// Call `fad_vec_free(buf, cap, pair_stride, pair_align)` to free the pairs buffer.
    ///
    /// Used on the success path after `from_pair_slice` has moved the pairs into the map.
    /// Does NOT branch to error exit (pairs free on success path, not error).
    pub fn emit_call_pairs_free(
        &mut self,
        free_fn: *const u8,
        buf_slot: u32,
        cap_slot: u32,
        pair_stride: u32,
        pair_align: u32,
    ) {
        let ptr_val = free_fn as i64;

        #[cfg(not(windows))]
        dynasm!(self.ops ; .arch x64
            ; mov rdi, [rsp + buf_slot as i32]
            ; mov rsi, [rsp + cap_slot as i32]
            ; mov edx, pair_stride as i32
            ; mov ecx, pair_align as i32
        );
        #[cfg(windows)]
        dynasm!(self.ops ; .arch x64
            ; mov rcx, [rsp + buf_slot as i32]
            ; mov rdx, [rsp + cap_slot as i32]
            ; mov r8d, pair_stride as i32
            ; mov r9d, pair_align as i32
        );
        dynasm!(self.ops ; .arch x64 ; mov rax, QWORD ptr_val ; call rax);
    }

    // ── Recipe emission ─────────────────────────────────────────────

    /// Emit a recipe — interpret a sequence of micro-ops into x86_64 instructions.
    pub fn emit_recipe(&mut self, recipe: &Recipe) {
        let error_exit = self.error_exit;

        // Allocate dynamic labels for the recipe
        let labels: Vec<DynamicLabel> = (0..recipe.label_count)
            .map(|_| self.ops.new_dynamic_label())
            .collect();

        // Shared EOF error label
        let eof_label = self.ops.new_dynamic_label();

        for op in &recipe.ops {
            match op {
                Op::BoundsCheck { count } => {
                    if *count == 1 {
                        dynasm!(self.ops
                            ; .arch x64
                            ; cmp r12, r13
                            ; jae =>eof_label
                        );
                    } else {
                        let count = *count as i32;
                        dynasm!(self.ops
                            ; .arch x64
                            ; mov r10, r13
                            ; sub r10, r12
                            ; cmp r10, count
                            ; jb =>eof_label
                        );
                    }
                }
                Op::LoadByte { dst } => match dst {
                    Slot::A => dynasm!(self.ops ; .arch x64 ; movzx r10d, BYTE [r12]),
                    Slot::B => dynasm!(self.ops ; .arch x64 ; movzx r11d, BYTE [r12]),
                },
                Op::LoadFromCursor { dst, width } => match (dst, width) {
                    (Slot::A, Width::W4) => dynasm!(self.ops ; .arch x64 ; mov r10d, DWORD [r12]),
                    (Slot::A, Width::W8) => dynasm!(self.ops ; .arch x64 ; mov r10, QWORD [r12]),
                    (Slot::B, Width::W4) => dynasm!(self.ops ; .arch x64 ; mov r11d, DWORD [r12]),
                    (Slot::B, Width::W8) => dynasm!(self.ops ; .arch x64 ; mov r11, QWORD [r12]),
                    _ => panic!("unsupported LoadFromCursor width"),
                },
                Op::StoreToOut { src, offset, width } => {
                    let offset = *offset as i32;
                    match (src, width) {
                        (Slot::A, Width::W1) => dynasm!(self.ops ; .arch x64 ; mov BYTE [r14 + offset], r10b),
                        (Slot::A, Width::W2) => dynasm!(self.ops ; .arch x64 ; mov WORD [r14 + offset], r10w),
                        (Slot::A, Width::W4) => dynasm!(self.ops ; .arch x64 ; mov DWORD [r14 + offset], r10d),
                        (Slot::A, Width::W8) => dynasm!(self.ops ; .arch x64 ; mov QWORD [r14 + offset], r10),
                        (Slot::B, Width::W1) => dynasm!(self.ops ; .arch x64 ; mov BYTE [r14 + offset], r11b),
                        (Slot::B, Width::W2) => dynasm!(self.ops ; .arch x64 ; mov WORD [r14 + offset], r11w),
                        (Slot::B, Width::W4) => dynasm!(self.ops ; .arch x64 ; mov DWORD [r14 + offset], r11d),
                        (Slot::B, Width::W8) => dynasm!(self.ops ; .arch x64 ; mov QWORD [r14 + offset], r11),
                    }
                }
                Op::StoreByteToStack { src, sp_offset } => {
                    let sp_offset = *sp_offset as i32;
                    match src {
                        Slot::A => dynasm!(self.ops ; .arch x64 ; mov BYTE [rsp + sp_offset], r10b),
                        Slot::B => dynasm!(self.ops ; .arch x64 ; mov BYTE [rsp + sp_offset], r11b),
                    }
                }
                Op::StoreToStack { src, sp_offset, width } => {
                    let sp_offset = *sp_offset as i32;
                    match (src, width) {
                        (Slot::A, Width::W4) => dynasm!(self.ops ; .arch x64 ; mov DWORD [rsp + sp_offset], r10d),
                        (Slot::A, Width::W8) => dynasm!(self.ops ; .arch x64 ; mov QWORD [rsp + sp_offset], r10),
                        (Slot::B, Width::W4) => dynasm!(self.ops ; .arch x64 ; mov DWORD [rsp + sp_offset], r11d),
                        (Slot::B, Width::W8) => dynasm!(self.ops ; .arch x64 ; mov QWORD [rsp + sp_offset], r11),
                        _ => panic!("unsupported StoreToStack width"),
                    }
                }
                Op::LoadFromStack { dst, sp_offset, width } => {
                    let sp_offset = *sp_offset as i32;
                    match (dst, width) {
                        (Slot::A, Width::W4) => dynasm!(self.ops ; .arch x64 ; mov r10d, DWORD [rsp + sp_offset]),
                        (Slot::A, Width::W8) => dynasm!(self.ops ; .arch x64 ; mov r10, QWORD [rsp + sp_offset]),
                        (Slot::B, Width::W4) => dynasm!(self.ops ; .arch x64 ; mov r11d, DWORD [rsp + sp_offset]),
                        (Slot::B, Width::W8) => dynasm!(self.ops ; .arch x64 ; mov r11, QWORD [rsp + sp_offset]),
                        _ => panic!("unsupported LoadFromStack width"),
                    }
                }
                Op::AdvanceCursor { count } => {
                    let count = *count as i32;
                    dynasm!(self.ops ; .arch x64 ; add r12, count);
                }
                Op::AdvanceCursorBySlot { slot } => match slot {
                    Slot::A => dynasm!(self.ops ; .arch x64 ; add r12, r10),
                    Slot::B => dynasm!(self.ops ; .arch x64 ; add r12, r11),
                },
                Op::ZigzagDecode { slot } => match slot {
                    Slot::A => dynasm!(self.ops
                        ; .arch x64
                        ; mov r11d, r10d
                        ; shr r11d, 1
                        ; and r10d, 1
                        ; neg r10d
                        ; xor r10d, r11d
                    ),
                    Slot::B => dynasm!(self.ops
                        ; .arch x64
                        ; mov r10d, r11d
                        ; shr r10d, 1
                        ; and r11d, 1
                        ; neg r11d
                        ; xor r11d, r10d
                    ),
                },
                Op::ValidateMax { slot, max_val, error } => {
                    let max_val = *max_val as i32;
                    let error_code = *error as i32;
                    let invalid_label = self.ops.new_dynamic_label();
                    let ok_label = self.ops.new_dynamic_label();
                    match slot {
                        Slot::A => dynasm!(self.ops
                            ; .arch x64
                            ; cmp r10d, max_val
                            ; ja =>invalid_label
                        ),
                        Slot::B => dynasm!(self.ops
                            ; .arch x64
                            ; cmp r11d, max_val
                            ; ja =>invalid_label
                        ),
                    }
                    dynasm!(self.ops
                        ; .arch x64
                        ; jmp =>ok_label
                        ; =>invalid_label
                        ; mov DWORD [r15 + CTX_ERROR_CODE as i32], error_code
                        ; jmp =>error_exit
                        ; =>ok_label
                    );
                }
                Op::TestBit7Branch { slot, target } => {
                    let label = labels[*target];
                    match slot {
                        Slot::A => dynasm!(self.ops
                            ; .arch x64
                            ; test r10d, 0x80
                            ; jnz =>label
                        ),
                        Slot::B => dynasm!(self.ops
                            ; .arch x64
                            ; test r11d, 0x80
                            ; jnz =>label
                        ),
                    }
                }
                Op::Branch { target } => {
                    let label = labels[*target];
                    dynasm!(self.ops ; .arch x64 ; jmp =>label);
                }
                Op::BindLabel { index } => {
                    let label = labels[*index];
                    dynasm!(self.ops ; .arch x64 ; =>label);
                }
                Op::CallIntrinsic { fn_ptr, field_offset } => {
                    let ptr_val = *fn_ptr as i64;
                    let field_offset = *field_offset as i32;
                    dynasm!(self.ops ; .arch x64 ; mov [r15 + CTX_INPUT_PTR as i32], r12);
                    #[cfg(not(windows))]
                    dynasm!(self.ops ; .arch x64 ; mov rdi, r15 ; lea rsi, [r14 + field_offset]);
                    #[cfg(windows)]
                    dynasm!(self.ops ; .arch x64 ; mov rcx, r15 ; lea rdx, [r14 + field_offset]);
                    dynasm!(self.ops
                        ; .arch x64
                        ; mov rax, QWORD ptr_val
                        ; call rax
                        ; mov r12, [r15 + CTX_INPUT_PTR as i32]
                        ; mov r10d, [r15 + CTX_ERROR_CODE as i32]
                        ; test r10d, r10d
                        ; jnz =>error_exit
                    );
                }
                Op::CallIntrinsicStackOut { fn_ptr, sp_offset } => {
                    let ptr_val = *fn_ptr as i64;
                    let sp_offset = *sp_offset as i32;
                    dynasm!(self.ops ; .arch x64 ; mov [r15 + CTX_INPUT_PTR as i32], r12);
                    #[cfg(not(windows))]
                    dynasm!(self.ops ; .arch x64 ; mov rdi, r15 ; lea rsi, [rsp + sp_offset]);
                    #[cfg(windows)]
                    dynasm!(self.ops ; .arch x64 ; mov rcx, r15 ; lea rdx, [rsp + sp_offset]);
                    dynasm!(self.ops
                        ; .arch x64
                        ; mov rax, QWORD ptr_val
                        ; call rax
                        ; mov r12, [r15 + CTX_INPUT_PTR as i32]
                        ; mov r11d, [r15 + CTX_ERROR_CODE as i32]
                        ; test r11d, r11d
                        ; jnz =>error_exit
                    );
                }
                Op::ComputeRemaining { dst } => match dst {
                    Slot::A => dynasm!(self.ops
                        ; .arch x64
                        ; mov r10, r13
                        ; sub r10, r12
                    ),
                    Slot::B => dynasm!(self.ops
                        ; .arch x64
                        ; mov r11, r13
                        ; sub r11, r12
                    ),
                },
                Op::CmpBranchLo { lhs, rhs, on_fail } => {
                    let target = match on_fail {
                        ErrorTarget::Eof => eof_label,
                        ErrorTarget::ErrorExit => error_exit,
                    };
                    match (lhs, rhs) {
                        (Slot::A, Slot::B) => dynasm!(self.ops
                            ; .arch x64
                            ; cmp r10, r11
                            ; jb =>target
                        ),
                        (Slot::B, Slot::A) => dynasm!(self.ops
                            ; .arch x64
                            ; cmp r11, r10
                            ; jb =>target
                        ),
                        _ => panic!("CmpBranchLo requires different slots"),
                    }
                }
                Op::SaveCursor { dst } => match dst {
                    Slot::A => dynasm!(self.ops ; .arch x64 ; mov r10, r12),
                    Slot::B => dynasm!(self.ops ; .arch x64 ; mov r11, r12),
                },
                Op::CallValidateAllocCopy { fn_ptr, data_src, len_src } => {
                    let ptr_val = *fn_ptr as i64;
                    // Call fad_string_validate_alloc_copy(ctx, data_ptr, data_len)
                    // Returns buf pointer in rax
                    // arg0 = ctx, arg1 = data_ptr (slot), arg2 = data_len (slot)
                    dynasm!(self.ops ; .arch x64 ; mov [r15 + CTX_INPUT_PTR as i32], r12);
                    #[cfg(not(windows))]
                    {
                        dynasm!(self.ops ; .arch x64 ; mov rdi, r15);
                        match data_src {
                            Slot::A => dynasm!(self.ops ; .arch x64 ; mov rsi, r10),
                            Slot::B => dynasm!(self.ops ; .arch x64 ; mov rsi, r11),
                        }
                        match len_src {
                            Slot::A => dynasm!(self.ops ; .arch x64 ; mov edx, r10d),
                            Slot::B => dynasm!(self.ops ; .arch x64 ; mov edx, r11d),
                        }
                    }
                    #[cfg(windows)]
                    {
                        dynasm!(self.ops ; .arch x64 ; mov rcx, r15);
                        match data_src {
                            Slot::A => dynasm!(self.ops ; .arch x64 ; mov rdx, r10),
                            Slot::B => dynasm!(self.ops ; .arch x64 ; mov rdx, r11),
                        }
                        match len_src {
                            Slot::A => dynasm!(self.ops ; .arch x64 ; mov r8d, r10d),
                            Slot::B => dynasm!(self.ops ; .arch x64 ; mov r8d, r11d),
                        }
                    }
                    dynasm!(self.ops
                        ; .arch x64
                        ; mov rax, QWORD ptr_val
                        ; call rax
                        ; mov r11d, [r15 + CTX_ERROR_CODE as i32]
                        ; test r11d, r11d
                        ; jnz =>error_exit
                    );
                }
                Op::WriteMalumString { base_offset, ptr_off, len_off, cap_off, len_slot } => {
                    let ptr_offset = (*base_offset + *ptr_off) as i32;
                    let len_offset = (*base_offset + *len_off) as i32;
                    let cap_offset = (*base_offset + *cap_off) as i32;
                    // rax = buf pointer from previous call return
                    dynasm!(self.ops ; .arch x64 ; mov [r14 + ptr_offset], rax);
                    match len_slot {
                        Slot::A => {
                            dynasm!(self.ops
                                ; .arch x64
                                ; mov [r14 + len_offset], r10
                                ; mov [r14 + cap_offset], r10
                            );
                        }
                        Slot::B => {
                            dynasm!(self.ops
                                ; .arch x64
                                ; mov [r14 + len_offset], r11
                                ; mov [r14 + cap_offset], r11
                            );
                        }
                    }
                }
            }
        }

        // Jump over cold path, then emit shared EOF error
        let done_label = self.ops.new_dynamic_label();
        let eof_code = crate::context::ErrorCode::UnexpectedEof as i32;
        dynasm!(self.ops
            ; .arch x64
            ; jmp =>done_label
            ; =>eof_label
            ; mov DWORD [r15 + CTX_ERROR_CODE as i32], eof_code
            ; jmp =>error_exit
            ; =>done_label
        );
    }

    /// Commit and finalize the assembler, returning the executable buffer.
    ///
    /// All functions must have been completed with `end_func` before calling this.
    pub fn finalize(mut self) -> dynasmrt::ExecutableBuffer {
        self.ops.commit().expect("failed to commit assembly");
        self.ops.finalize().expect("failed to finalize assembly")
    }
}
